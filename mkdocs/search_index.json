{
    "docs": [
        {
            "location": "/", 
            "text": "Aether\n\n\nAether is an intuitive, easy to use, cost-effective, and scalable framework that\nuses linear programming (LP) to optimally bid on and deploy combinations of\nunderutilized cloud computing resources. Our approach simultaneously minimizes\nthe cost of data analysis while maximizing efficiency and speed.\nThrough its optimized instance bidding, Aether is able to reduce cloud computing costs by nearly ninety percent.\nAether was developed jointly by the \nKostic\n and\n\nPatel\n labs at Harvard Medical School. Source\ncode is available \nhere\n.\n\n\n\n\nInstallation and Configuration\n\n\nDetailed instructions for software setup as well as additional documentation are available \nin the tutorial section\n.\n\n\n\n\nUsing Aether\n\n\nA step-by-step tutorial for using Aether is located \nin the tutorial section\n.\nIf you have further questions about Aether, you may want to file an issue on \nGitHub\n.\nBefore filing an issue, make sure to read our \ncontribution guidelines\n and use our \nissue template\n.\n\n\n\n\nPublication\n\n\nAether is published in OUP Bioinformatics. Please cite our manuscript if you use\nAether:\n\n\nJacob M Luber, Braden T Tierney, Evan M Cofer, Chirag J Patel, Aleksandar D Kostic; Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics, Bioinformatics,btx787, \nhttps://doi.org/10.1093/bioinformatics/btx787\n\n\n\n\nHelp\n\n\nIf you have questions, consider reading the \ntutorials\n or \nfrequently asked questions\n.\nAlternatively, you can reach out to us following the instructions \nhere\n.\nGuidelines for filing feature requests, documentation requests, bug reports, and general issues are also available \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#aether", 
            "text": "Aether is an intuitive, easy to use, cost-effective, and scalable framework that\nuses linear programming (LP) to optimally bid on and deploy combinations of\nunderutilized cloud computing resources. Our approach simultaneously minimizes\nthe cost of data analysis while maximizing efficiency and speed.\nThrough its optimized instance bidding, Aether is able to reduce cloud computing costs by nearly ninety percent.\nAether was developed jointly by the  Kostic  and Patel  labs at Harvard Medical School. Source\ncode is available  here .", 
            "title": "Aether"
        }, 
        {
            "location": "/#installation_and_configuration", 
            "text": "Detailed instructions for software setup as well as additional documentation are available  in the tutorial section .", 
            "title": "Installation and Configuration"
        }, 
        {
            "location": "/#using_aether", 
            "text": "A step-by-step tutorial for using Aether is located  in the tutorial section .\nIf you have further questions about Aether, you may want to file an issue on  GitHub .\nBefore filing an issue, make sure to read our  contribution guidelines  and use our  issue template .", 
            "title": "Using Aether"
        }, 
        {
            "location": "/#publication", 
            "text": "Aether is published in OUP Bioinformatics. Please cite our manuscript if you use\nAether:  Jacob M Luber, Braden T Tierney, Evan M Cofer, Chirag J Patel, Aleksandar D Kostic; Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics, Bioinformatics,btx787,  https://doi.org/10.1093/bioinformatics/btx787", 
            "title": "Publication"
        }, 
        {
            "location": "/#help", 
            "text": "If you have questions, consider reading the  tutorials  or  frequently asked questions .\nAlternatively, you can reach out to us following the instructions  here .\nGuidelines for filing feature requests, documentation requests, bug reports, and general issues are also available  here .", 
            "title": "Help"
        }, 
        {
            "location": "/tutorials/", 
            "text": "Prerequisite Skills\n\n\nTo follow along with this tutorial, you should have working knowledge of the Unix command line and Bash.\nIf you are not familiar with these, we have included links to several free tutorials below.\nNote that they are sorted by time investment and in ascending order.\n\n\n\n\nUnix Basics for Bioinformatics\n\n\nJust Enough Unix for Bioinformatics\n\n\nCommand Line Bootcamp\n\n\nAdvanced Bash-Scripting Guide\n\n\n\n\nYou should also be somewhat familiar with Amazon Web Services (AWS), and have an AWS account.\nIf you do not feel comfortable with using AWS, we recommend that you consider reading through \nthis tutorial\n.\nAn Azure account may also be used.\nYou can learn more about using Azure \nhere\n.\n\n\n\n\nPrerequisite Software\n\n\nTo run Aether, you will need a copy of Python (Version 2.7).\nIf you have not already installed Python, we recommend installing \nAnaconda\n, which comes with Python and several core packages.\nInstructions for installing Anaconda are available \nhere\n.\n\n\nWith Anaconda\n\n\nIf you have Anaconda, dependencies for Aether are handled automatically during the installation process.\n\n\nWithout Anaconda\n\n\nIf you do not use Anaconda, you will have to install dependencies manually.\nThis is not recommended.\nThese dependencies are:\n\n\n\n\nPython 2.7\n\n\nAWS Command Line Interface\n\n\nAzure Command Line Interface\n\n\njq\n\n\nscipy\n\n\nnumpy\n\n\npip\n\n\nClick\n\n\n\n\nIf you would like to build the documentation, you will also have to install \nMkDocs\n.\n\n\n\n\nConfiguration and Setup\n\n\nIn this section, you will find a detailed explanation of how to install or uninstall Aether.\nWe also describe how to build the documentation page on your own machine.\n\n\n\n\nInstallation\n\n\nTo install Aether, you first need to acquire a copy of it from \nGitHub\n.\nTo do this, you may either clone the git repository, or download an archive.\nTo clone the git repository, run the following in the command line:\n\n\ngit clone git@github.com:kosticlab/aether.git\n\n\n\n\nAlternatively, you can download an archive of Aether \nhere\n.\nTo unpack the archive, run the following:\n\n\ntar -xvf aether-master.tar.gz\nmv aether-master aether\n\n\n\n\nOnce you have acquired a copy of Aether, open the directory for Aether.\nWe herein refer to this directory, or folder, as the \"Aether directory\".\nIf you have Anaconda installed, simply run:\n\n\nmake build\n\n\n\n\nIf you do not have Anaconda installed, instead run:\n\n\nmake build-noconda\n\n\n\n\n\n\nBuilding Documentation\n\n\nIf you would like to build the documentation for Aether on your machine, run the following commands while in the Aether directory:\n\n\nmake docs\n\n\n\n\nNote that some additional instructions for viewing the docs will be printed to your console when you run this command.\n\n\n\n\nRemoval\n\n\nIf you would like to uninstall Aether from your machine, simply run the following commands while in the Aether directory:\n\n\nmake clean\n\n\n\n\nOnce the Aether package has been uninstalled, simply delete the Aether directory that you downloaded from GitHub.\n\n\n\n\nRunning Aether\n\n\nThis section contains a detailed overview of Aether and how it functions.\nIt also contains explanations of the various data used by Aether.\n\n\n\n\nExecution Modes for Aether\n\n\nAether can be run in several different modes.\nThe three primary modes are interactive mode, non-interactive mode, and \"dry run\" mode.\nThe interactive mode will prompt the user for the various inputs, whereas the non-interactive mode will pull this information from the command line arguments.\nLastly, the \"dry run\" mode will show the user what resources Aether would have bid on, as well as their cost.\nHowever, \"dry run\" mode will not actually run anything on the cloud, and thus will not result in any cloud resources being used.\nInformation about running Aether with these modes is located below.\n\n\nInteractive Mode\n\n\nWhen Aether is run in interactive mode, it will prompt you for the various\nprogram parameters.\nBecause Aether's optimization method requires a number of parameters, the\ninteractive mode is highly recommended for new users.\nTo run interactive mode, simply run the following in the Aether directory:\n\n\naether --interactive [ARGS]\n\n\n\n\nOr\n\n\naether --I [ARGS]\n\n\n\n\nNon-Interactive Mode\n\n\nThe non-interactive mode will not prompt the user for any input information.\nNon-interactive mode is generally not recommended for new users.\nTo run Aether in non-interactive mode, simply run the following in the Aether\ndirectory:\n\n\naether [ARGS]\n\n\n\n\nDry-Run Mode\n\n\nThe dry-run mode will show the user what bids Aether suggests, but will not use\nany cloud resources.\nTo run Aether in dry-run mode, simply run the following in the Aether directory:\n\n\naether --dry-run [ARGS]\n\n\n\n\nNote that you can run the Dry-Run mode in interactive mode by just adding the\n\n--interactive\n or \n-I\n argument to this command.\n\n\nCommand Line Arguments\n\n\nAs always, additional information about command line arguments for may be found by running:\n\n\naether --help\n\n\n\n\nThis will output the following:\n\n\nUsage: aether [OPTIONS]\n\n  The Aether Command Line Interface\n\nOptions:\n  -I, --interactive             Enables interactive mode.\n  --dry-run                     Runs Aether in dry-run mode. This shows what\n                                cloud computing resources Aether would use,\n                                but does not actually use them or perform any\n                                computation.\n  -A, --input-file TEXT         The name of a text file, wherein each line\n                                corresponds to an argument passed to one of\n                                the distributed batch jobs.\n  -L, --provisioning-file TEXT  Filename of the provisioning file.\n  -P, --processors INTEGER      The number of cores that each batch job\n                                requires\n  -M, --memory INTEGER          The amount of memory, in Gigabytes, that each\n                                batch job will require.\n  -N, --name TEXT               The name of the project. This should be\n                                unique, as an S3 bucket is created on Amazon\n                                for this project, and they must have unique\n                                names.\n  -E, --key-ID TEXT             Cloud CLI Access Key ID.\n  -K, --key TEXT                Cloud CLI Access Key.\n  -R, --region TEXT             The region/datacenter that the pipeline should\n                                be run in (e.g. \nus-east-1\n).\n  -B, --bin-dir TEXT            The directory with applications runnable on\n                                the cloud image that are dependencies for your\n                                batch jobs. Paths in your scripts must be\n                                reachable from the top level of this\n                                directory.\n  -S, --script TEXT             The script to be run for every line in input-\n                                file and distributed across the cluster.\n  -D, --data TEXT               The directory of any data that the job script\n                                will need to access.\n  --help                        Show this message and exit.\n\n\n\n\n\n\nFinding Account Information Required to Run Aether\n\n\nNot all of the data that Aether needs to run can be securely accessed automatically.\nIn particular, we do not access your private AWS account information, and instead require the user to input this information.\nWe provide details on how to find this data in the sections below.\n\n\n\n\nAccess Key Information\n\n\nInstructions for locating your AWS Access Key ID and Access Key can be found \nhere\n.\n\n\n\n\nInstance Limits Information\n\n\nWhen you run Aether, you will be prompted for some information on account limits, as AWS does not allow them to be programmatically retrieved.\nIn the \n.gif\n below, we show a demonstration of where to access this information on the \nAWS website\n.\n\n\n\n\nThese account limits are automatically saved in the \ninstances.p\n file, and may be entered into the bidder on subsequent runs to save time.\n\n\nOnce you have entered account limits into Aether, it will begin solving the multi-objective optimization problem of selecting the optimal bidding strategy.\nThis is a computationally intensive process, during which Aether is iteratively performing a number of high-dimensional convex optimizations.\nOn a lightweight computer (e.g., an older laptop), this may impact performance of other programs that are running.\n\n\n\n\nCloud Resource Provisioning Information\n\n\nAfter running Aether, you will find that it has generated a new file, named \nprov.psv\n.\nThis file contains the provisioning information for the batch processing pipeline, which is the second component of Aether.\nWe turn now to the details of \nprov.psv\n.\nEach line in \nprov.psv\n is a list delimited by the vertical bar character.\nIn order, the columns represent\nTYPE\n, \nPROCESSORS\n, \nRAM\n, \nSTORAGE\n, \nBIDDABLE\n, which we explain in the table below.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nTYPE\n\n\nThe type of instance that is being requested. This is a name defined by the provider.\n\n\n\n\n\n\nPROCESSORS\n\n\nThe number of processors that are available to this type of instance.\n\n\n\n\n\n\nRAM\n\n\nThe amount of Random Access Memory (RAM) that is available to this type of instance.\n\n\n\n\n\n\nSTORAGE\n\n\nWhether there is ephemeral storage available to this instance during use.\n\n\n\n\n\n\nBIDDABLE\n\n\nWhether this instance should be bid upon or purchased at its standard market price.\n\n\n\n\n\n\n\n\nAs an example, if \nprov.psv\n contains the following:\n\n\nc4.large|2|3|false|true\nc4.large|2|3|false|false\n\n\n\n\nThen Aether would instruct the batch processing pipeline to spin up two \nc4.large\n instances, purchasing one at market cost and bidding on the second one.\nAs an aside, it is possible to run the batch processing pipeline without the bidder if you already have \nprov.psv\n (e.g., generated it manually, reusing it).\nTo do so, simply replace \n--file=prov.psv\n with \n--file=YourFileHere.psv\n, where \nYourFileHere.psv\n is the name of your \n.psv\n file with the provisioning information in it.\n\n\n\n\nJob Run-Time Environment and Resources Available by Default\n\n\nEach time you submit a batch of jobs to Aether, it provides some information about the environment to the job that is being executed.\nAn example of this is seen below.\nA copy of this template is also located in \nexamples/template\n.\nAdditionally, the \nbin\n folder that was uploaded to the cloud should be available to the job script at \nbin\n.\nNote that this is not the global \n/bin\n, but instead relative to the job script's execution location.\n\n\n#!/bin/bash\nPASSEDARGUMENT=$1\nPROCS=$2\nMEMFRAC=$3\nPRIMARYHOST=$4\nOUTPUTFOLDER=$5\nLOCALIP=$6\n# Code goes here (likely includes uploading of results to s3 bucket)\n\n# Access uploaded scripts, programs, and binaries e.g. \npython\n#bin/uploaded_script_name.py\n or \nbin/upload_script_name.sh\n\n\n#this script should always conclude with the line below in order to tell the\nmaster node to assign a new job.\nbin/terminate_job.sh $6 $1 $4\n\n\n\n\n\nDetails about these parameters are shown in the table below.\n\n\n\n\n\n\n\n\nParameter\n\n\nInformation\n\n\n\n\n\n\n\n\n\n\nPASSEDARGUMENT\n\n\nThe string representing an argument for an individual batch job\n\n\n\n\n\n\nPROCS\n\n\nThe number of processors that this task can safely utilize.\n\n\n\n\n\n\nMEMFRAC\n\n\nThe memory fraction of the current node that this task can safely utilize.\n\n\n\n\n\n\nPRIMARYHOST\n\n\nThe static IP address of the node controlling batch processing. Will be notified upon task completion.\n\n\n\n\n\n\nOUTPUTFOLDER\n\n\nThe location in cloud file system to store the job outputs.\n\n\n\n\n\n\nLOCALIP\n\n\nThe local IP of this machine.\n\n\n\n\n\n\n\n\nFinally, note that any item in the DATA folder may be downloaded from the cloud file system (e.g., S3) during job run-time using the AWS or Azure Command Line Interface.\nLocations could be part of pre-generated arguments, as the S3 bucket is named the same as previously chosen project name.\n\n\n\n\nAcessing Data Storage During Job Run-Time\n\n\nTo add storage to a batch node for jobs require large amounts of disk space,\nrun:\n\n\nbin/add_ebs_storage.sh SIZE DEVMNT\n\n\n\n\nDetails about these parameters are shown in the table below.\n\n\n\n\n\n\n\n\nParameter\n\n\nInformation\n\n\n\n\n\n\n\n\n\n\nSIZE\n\n\nThe number representing the size in GB of the SSD to attach.\n\n\n\n\n\n\nDEVMNT\n\n\nThe location in /dev to be created and used to mount the disk.\n\n\n\n\n\n\n\n\nThis utility should be run from within the batch job processing script from the\nprevious section, likely including a check to see whether or not the desired\ndisk has been previously mounted.\n\n\n\n\nExamples\n\n\nWe have included several examples of using Aether below.\nBecause distributed systems are inherently complex and nondeterministic, we strongly recommend reading through these examples before running Aether on your own.\n\n\n\n\nExample: Basic Use\n\n\nIn \nexamples/basic\n there exists \nargs.txt\n and \ntest.sh\n. If the batch\nprocessing script is run with interactive mode with \nargs.txt\n as the input\nfile argument and \ntest.sh\n as the script argument the bidder will be run and\non the distributed compute that is spun up each replica node will upon receipt of a task  write the passed line from\n\nargs.txt\n to a file, wait for 30 seconds, upload the file to S3, and then\ncommunicate to the primary node that the job is complete and that another job\ncan begin. If there are no new jobs then the instances terminate themselves\nautomatically.\n\n\nAccessing Output\n\n\nOutput can be accessed in an S3 bucket that bears the same name as the project\npassed in via the name argument, e.g. \ns3://projectname\n. Logs that are\nautomatically generated are also placed in this bucket.\n\n\n\n\nExample: Assembling Metagenomes (in paper)\n\n\nThis is the example use of the pipeline that is presented in the paper that\ndemonstrates more complex uses. This\nexample is located in \nexamples/metagenome_assembly\n. To run this example in\ninteractive mode,\nplace \nassemble_sample.py\n in a folder with built versions of\n\nprokka\n and\n\nmegahit\n and pass the location of this\nfolder as the bin-dir argument. Pass \nbatch_script.sh\n, which essentially is\njust a wrapper to run the python script in \nbin\n, as the script argument. Pass\nthe location of a folder containing fastq files of paired end metagenomic reads\nas the data argument. Finally, for the input file argument, pass a text file\nwhere each line contains the s3 locations (parameterized by name given to\nproject) of 2 matching paired end reads\nseparated by a comma, e.g.\n\ns3://nameofproject/data/samplexpe1.fastq.gz,s3://nameofproject/data/samplexpe2.fast1.gz\n. For the analysis done in the paper 1572 samples were assembled but this script can be used on any number of samples.\n\n\n\n\nAdvanced Features\n\n\nThis section discusses a number of Aether's useful, but advanced, features.\nHowever, many of these features have complexities or unique aspects that make them less straightforward or more involved than those discussed thus far.\nAs such, it is recommended that you first read through the documentation above.\n\n\n\n\nAdd A Microsoft Azure Machine To A Running Batch Processing Pipeline\n\n\nTo add a Microsoft azure node to a currently executing Aether pipeline, run \nbin/az.sh [args]\n where \n[args]\n are\nthe following 21 arguments in sequential order:\n\n\n\n\n\n\n\n\nArgument Number\n\n\nInformation\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAzure Username\n\n\n\n\n\n\n2\n\n\nAzure Password\n\n\n\n\n\n\n3\n\n\nAzure Subscription ID\n\n\n\n\n\n\n4\n\n\nAzure Resource Group Name\n\n\n\n\n\n\n5\n\n\nAzure Location\n\n\n\n\n\n\n6\n\n\nAzure Public IP Identifier\n\n\n\n\n\n\n7\n\n\nAzure DNS Identifier\n\n\n\n\n\n\n8\n\n\nAzure Virtual Network Name\n\n\n\n\n\n\n9\n\n\nAzure Subnet Name\n\n\n\n\n\n\n10\n\n\nAzure NIC Name\n\n\n\n\n\n\n11\n\n\nAzure Virtual Machine Name\n\n\n\n\n\n\n12\n\n\nAWS CLI Key ID\n\n\n\n\n\n\n13\n\n\nAWS CLI Key\n\n\n\n\n\n\n14\n\n\nAWS Region\n\n\n\n\n\n\n15\n\n\nBatch Jobs Azure VM Can Handle Concurrently\n\n\n\n\n\n\n16\n\n\nProcessors Needed Per Batch Job\n\n\n\n\n\n\n17\n\n\nFraction Of Node Memory That One Task Will Utilize\n\n\n\n\n\n\n18\n\n\nS3 Location Where Output Should Be Uploaded To\n\n\n\n\n\n\n19\n\n\nStatic IP Of Primary Node\n\n\n\n\n\n\n20\n\n\nFor Compatibility With Scripts For AWS Always Make This Argument \"false\"\n\n\n\n\n\n\n21\n\n\nType Of Azure VM To Spin Up\n\n\n\n\n\n\n\n\n\n\nAdd Your Local Machine To A Running Batch Processing Pipeline\n\n\nPlease note that it is a requirement that your hardware has a static IP address and is not being operated under any sort of scheduler.\nTo add your own hardware to a currently executing Aether pipeline, run\n\nbin/local.sh [args]\n where \n[args]\n are\nthe following 9 arguments in sequential order:\n\n\n\n\n\n\n\n\nArgument Number\n\n\nInformation\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAWS CLI Key ID\n\n\n\n\n\n\n2\n\n\nAWS CLI Key\n\n\n\n\n\n\n3\n\n\nAWS Region\n\n\n\n\n\n\n4\n\n\nBatch Jobs Your Hardware Can Handle Concurrently\n\n\n\n\n\n\n5\n\n\nProcessors Needed Per Batch Job\n\n\n\n\n\n\n6\n\n\nFraction Of Node Memory That One Task Will Utilize\n\n\n\n\n\n\n7\n\n\nS3 Location Where Output Should Be Uploaded To\n\n\n\n\n\n\n8\n\n\nStatic IP Of Primary Node\n\n\n\n\n\n\n9\n\n\nFor Compatibility With Scripts For AWS Always Make This Argument \"false\"", 
            "title": "Information and Tutorials"
        }, 
        {
            "location": "/tutorials/#prerequisite_skills", 
            "text": "To follow along with this tutorial, you should have working knowledge of the Unix command line and Bash.\nIf you are not familiar with these, we have included links to several free tutorials below.\nNote that they are sorted by time investment and in ascending order.   Unix Basics for Bioinformatics  Just Enough Unix for Bioinformatics  Command Line Bootcamp  Advanced Bash-Scripting Guide   You should also be somewhat familiar with Amazon Web Services (AWS), and have an AWS account.\nIf you do not feel comfortable with using AWS, we recommend that you consider reading through  this tutorial .\nAn Azure account may also be used.\nYou can learn more about using Azure  here .", 
            "title": "Prerequisite Skills"
        }, 
        {
            "location": "/tutorials/#prerequisite_software", 
            "text": "To run Aether, you will need a copy of Python (Version 2.7).\nIf you have not already installed Python, we recommend installing  Anaconda , which comes with Python and several core packages.\nInstructions for installing Anaconda are available  here .", 
            "title": "Prerequisite Software"
        }, 
        {
            "location": "/tutorials/#with_anaconda", 
            "text": "If you have Anaconda, dependencies for Aether are handled automatically during the installation process.", 
            "title": "With Anaconda"
        }, 
        {
            "location": "/tutorials/#without_anaconda", 
            "text": "If you do not use Anaconda, you will have to install dependencies manually.\nThis is not recommended.\nThese dependencies are:   Python 2.7  AWS Command Line Interface  Azure Command Line Interface  jq  scipy  numpy  pip  Click   If you would like to build the documentation, you will also have to install  MkDocs .", 
            "title": "Without Anaconda"
        }, 
        {
            "location": "/tutorials/#configuration_and_setup", 
            "text": "In this section, you will find a detailed explanation of how to install or uninstall Aether.\nWe also describe how to build the documentation page on your own machine.", 
            "title": "Configuration and Setup"
        }, 
        {
            "location": "/tutorials/#installation", 
            "text": "To install Aether, you first need to acquire a copy of it from  GitHub .\nTo do this, you may either clone the git repository, or download an archive.\nTo clone the git repository, run the following in the command line:  git clone git@github.com:kosticlab/aether.git  Alternatively, you can download an archive of Aether  here .\nTo unpack the archive, run the following:  tar -xvf aether-master.tar.gz\nmv aether-master aether  Once you have acquired a copy of Aether, open the directory for Aether.\nWe herein refer to this directory, or folder, as the \"Aether directory\".\nIf you have Anaconda installed, simply run:  make build  If you do not have Anaconda installed, instead run:  make build-noconda", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorials/#building_documentation", 
            "text": "If you would like to build the documentation for Aether on your machine, run the following commands while in the Aether directory:  make docs  Note that some additional instructions for viewing the docs will be printed to your console when you run this command.", 
            "title": "Building Documentation"
        }, 
        {
            "location": "/tutorials/#removal", 
            "text": "If you would like to uninstall Aether from your machine, simply run the following commands while in the Aether directory:  make clean  Once the Aether package has been uninstalled, simply delete the Aether directory that you downloaded from GitHub.", 
            "title": "Removal"
        }, 
        {
            "location": "/tutorials/#running_aether", 
            "text": "This section contains a detailed overview of Aether and how it functions.\nIt also contains explanations of the various data used by Aether.", 
            "title": "Running Aether"
        }, 
        {
            "location": "/tutorials/#execution_modes_for_aether", 
            "text": "Aether can be run in several different modes.\nThe three primary modes are interactive mode, non-interactive mode, and \"dry run\" mode.\nThe interactive mode will prompt the user for the various inputs, whereas the non-interactive mode will pull this information from the command line arguments.\nLastly, the \"dry run\" mode will show the user what resources Aether would have bid on, as well as their cost.\nHowever, \"dry run\" mode will not actually run anything on the cloud, and thus will not result in any cloud resources being used.\nInformation about running Aether with these modes is located below.", 
            "title": "Execution Modes for Aether"
        }, 
        {
            "location": "/tutorials/#interactive_mode", 
            "text": "When Aether is run in interactive mode, it will prompt you for the various\nprogram parameters.\nBecause Aether's optimization method requires a number of parameters, the\ninteractive mode is highly recommended for new users.\nTo run interactive mode, simply run the following in the Aether directory:  aether --interactive [ARGS]  Or  aether --I [ARGS]", 
            "title": "Interactive Mode"
        }, 
        {
            "location": "/tutorials/#non-interactive_mode", 
            "text": "The non-interactive mode will not prompt the user for any input information.\nNon-interactive mode is generally not recommended for new users.\nTo run Aether in non-interactive mode, simply run the following in the Aether\ndirectory:  aether [ARGS]", 
            "title": "Non-Interactive Mode"
        }, 
        {
            "location": "/tutorials/#dry-run_mode", 
            "text": "The dry-run mode will show the user what bids Aether suggests, but will not use\nany cloud resources.\nTo run Aether in dry-run mode, simply run the following in the Aether directory:  aether --dry-run [ARGS]  Note that you can run the Dry-Run mode in interactive mode by just adding the --interactive  or  -I  argument to this command.", 
            "title": "Dry-Run Mode"
        }, 
        {
            "location": "/tutorials/#command_line_arguments", 
            "text": "As always, additional information about command line arguments for may be found by running:  aether --help  This will output the following:  Usage: aether [OPTIONS]\n\n  The Aether Command Line Interface\n\nOptions:\n  -I, --interactive             Enables interactive mode.\n  --dry-run                     Runs Aether in dry-run mode. This shows what\n                                cloud computing resources Aether would use,\n                                but does not actually use them or perform any\n                                computation.\n  -A, --input-file TEXT         The name of a text file, wherein each line\n                                corresponds to an argument passed to one of\n                                the distributed batch jobs.\n  -L, --provisioning-file TEXT  Filename of the provisioning file.\n  -P, --processors INTEGER      The number of cores that each batch job\n                                requires\n  -M, --memory INTEGER          The amount of memory, in Gigabytes, that each\n                                batch job will require.\n  -N, --name TEXT               The name of the project. This should be\n                                unique, as an S3 bucket is created on Amazon\n                                for this project, and they must have unique\n                                names.\n  -E, --key-ID TEXT             Cloud CLI Access Key ID.\n  -K, --key TEXT                Cloud CLI Access Key.\n  -R, --region TEXT             The region/datacenter that the pipeline should\n                                be run in (e.g.  us-east-1 ).\n  -B, --bin-dir TEXT            The directory with applications runnable on\n                                the cloud image that are dependencies for your\n                                batch jobs. Paths in your scripts must be\n                                reachable from the top level of this\n                                directory.\n  -S, --script TEXT             The script to be run for every line in input-\n                                file and distributed across the cluster.\n  -D, --data TEXT               The directory of any data that the job script\n                                will need to access.\n  --help                        Show this message and exit.", 
            "title": "Command Line Arguments"
        }, 
        {
            "location": "/tutorials/#finding_account_information_required_to_run_aether", 
            "text": "Not all of the data that Aether needs to run can be securely accessed automatically.\nIn particular, we do not access your private AWS account information, and instead require the user to input this information.\nWe provide details on how to find this data in the sections below.", 
            "title": "Finding Account Information Required to Run Aether"
        }, 
        {
            "location": "/tutorials/#access_key_information", 
            "text": "Instructions for locating your AWS Access Key ID and Access Key can be found  here .", 
            "title": "Access Key Information"
        }, 
        {
            "location": "/tutorials/#instance_limits_information", 
            "text": "When you run Aether, you will be prompted for some information on account limits, as AWS does not allow them to be programmatically retrieved.\nIn the  .gif  below, we show a demonstration of where to access this information on the  AWS website .   These account limits are automatically saved in the  instances.p  file, and may be entered into the bidder on subsequent runs to save time.  Once you have entered account limits into Aether, it will begin solving the multi-objective optimization problem of selecting the optimal bidding strategy.\nThis is a computationally intensive process, during which Aether is iteratively performing a number of high-dimensional convex optimizations.\nOn a lightweight computer (e.g., an older laptop), this may impact performance of other programs that are running.", 
            "title": "Instance Limits Information"
        }, 
        {
            "location": "/tutorials/#cloud_resource_provisioning_information", 
            "text": "After running Aether, you will find that it has generated a new file, named  prov.psv .\nThis file contains the provisioning information for the batch processing pipeline, which is the second component of Aether.\nWe turn now to the details of  prov.psv .\nEach line in  prov.psv  is a list delimited by the vertical bar character.\nIn order, the columns represent TYPE ,  PROCESSORS ,  RAM ,  STORAGE ,  BIDDABLE , which we explain in the table below.     Column Name  Definition      TYPE  The type of instance that is being requested. This is a name defined by the provider.    PROCESSORS  The number of processors that are available to this type of instance.    RAM  The amount of Random Access Memory (RAM) that is available to this type of instance.    STORAGE  Whether there is ephemeral storage available to this instance during use.    BIDDABLE  Whether this instance should be bid upon or purchased at its standard market price.     As an example, if  prov.psv  contains the following:  c4.large|2|3|false|true\nc4.large|2|3|false|false  Then Aether would instruct the batch processing pipeline to spin up two  c4.large  instances, purchasing one at market cost and bidding on the second one.\nAs an aside, it is possible to run the batch processing pipeline without the bidder if you already have  prov.psv  (e.g., generated it manually, reusing it).\nTo do so, simply replace  --file=prov.psv  with  --file=YourFileHere.psv , where  YourFileHere.psv  is the name of your  .psv  file with the provisioning information in it.", 
            "title": "Cloud Resource Provisioning Information"
        }, 
        {
            "location": "/tutorials/#job_run-time_environment_and_resources_available_by_default", 
            "text": "Each time you submit a batch of jobs to Aether, it provides some information about the environment to the job that is being executed.\nAn example of this is seen below.\nA copy of this template is also located in  examples/template .\nAdditionally, the  bin  folder that was uploaded to the cloud should be available to the job script at  bin .\nNote that this is not the global  /bin , but instead relative to the job script's execution location.  #!/bin/bash\nPASSEDARGUMENT=$1\nPROCS=$2\nMEMFRAC=$3\nPRIMARYHOST=$4\nOUTPUTFOLDER=$5\nLOCALIP=$6\n# Code goes here (likely includes uploading of results to s3 bucket)\n\n# Access uploaded scripts, programs, and binaries e.g.  python\n#bin/uploaded_script_name.py  or  bin/upload_script_name.sh \n\n#this script should always conclude with the line below in order to tell the\nmaster node to assign a new job.\nbin/terminate_job.sh $6 $1 $4  Details about these parameters are shown in the table below.     Parameter  Information      PASSEDARGUMENT  The string representing an argument for an individual batch job    PROCS  The number of processors that this task can safely utilize.    MEMFRAC  The memory fraction of the current node that this task can safely utilize.    PRIMARYHOST  The static IP address of the node controlling batch processing. Will be notified upon task completion.    OUTPUTFOLDER  The location in cloud file system to store the job outputs.    LOCALIP  The local IP of this machine.     Finally, note that any item in the DATA folder may be downloaded from the cloud file system (e.g., S3) during job run-time using the AWS or Azure Command Line Interface.\nLocations could be part of pre-generated arguments, as the S3 bucket is named the same as previously chosen project name.", 
            "title": "Job Run-Time Environment and Resources Available by Default"
        }, 
        {
            "location": "/tutorials/#acessing_data_storage_during_job_run-time", 
            "text": "To add storage to a batch node for jobs require large amounts of disk space,\nrun:  bin/add_ebs_storage.sh SIZE DEVMNT  Details about these parameters are shown in the table below.     Parameter  Information      SIZE  The number representing the size in GB of the SSD to attach.    DEVMNT  The location in /dev to be created and used to mount the disk.     This utility should be run from within the batch job processing script from the\nprevious section, likely including a check to see whether or not the desired\ndisk has been previously mounted.", 
            "title": "Acessing Data Storage During Job Run-Time"
        }, 
        {
            "location": "/tutorials/#examples", 
            "text": "We have included several examples of using Aether below.\nBecause distributed systems are inherently complex and nondeterministic, we strongly recommend reading through these examples before running Aether on your own.", 
            "title": "Examples"
        }, 
        {
            "location": "/tutorials/#example_basic_use", 
            "text": "In  examples/basic  there exists  args.txt  and  test.sh . If the batch\nprocessing script is run with interactive mode with  args.txt  as the input\nfile argument and  test.sh  as the script argument the bidder will be run and\non the distributed compute that is spun up each replica node will upon receipt of a task  write the passed line from args.txt  to a file, wait for 30 seconds, upload the file to S3, and then\ncommunicate to the primary node that the job is complete and that another job\ncan begin. If there are no new jobs then the instances terminate themselves\nautomatically.", 
            "title": "Example: Basic Use"
        }, 
        {
            "location": "/tutorials/#accessing_output", 
            "text": "Output can be accessed in an S3 bucket that bears the same name as the project\npassed in via the name argument, e.g.  s3://projectname . Logs that are\nautomatically generated are also placed in this bucket.", 
            "title": "Accessing Output"
        }, 
        {
            "location": "/tutorials/#example_assembling_metagenomes_in_paper", 
            "text": "This is the example use of the pipeline that is presented in the paper that\ndemonstrates more complex uses. This\nexample is located in  examples/metagenome_assembly . To run this example in\ninteractive mode,\nplace  assemble_sample.py  in a folder with built versions of prokka  and megahit  and pass the location of this\nfolder as the bin-dir argument. Pass  batch_script.sh , which essentially is\njust a wrapper to run the python script in  bin , as the script argument. Pass\nthe location of a folder containing fastq files of paired end metagenomic reads\nas the data argument. Finally, for the input file argument, pass a text file\nwhere each line contains the s3 locations (parameterized by name given to\nproject) of 2 matching paired end reads\nseparated by a comma, e.g. s3://nameofproject/data/samplexpe1.fastq.gz,s3://nameofproject/data/samplexpe2.fast1.gz . For the analysis done in the paper 1572 samples were assembled but this script can be used on any number of samples.", 
            "title": "Example: Assembling Metagenomes (in paper)"
        }, 
        {
            "location": "/tutorials/#advanced_features", 
            "text": "This section discusses a number of Aether's useful, but advanced, features.\nHowever, many of these features have complexities or unique aspects that make them less straightforward or more involved than those discussed thus far.\nAs such, it is recommended that you first read through the documentation above.", 
            "title": "Advanced Features"
        }, 
        {
            "location": "/tutorials/#add_a_microsoft_azure_machine_to_a_running_batch_processing_pipeline", 
            "text": "To add a Microsoft azure node to a currently executing Aether pipeline, run  bin/az.sh [args]  where  [args]  are\nthe following 21 arguments in sequential order:     Argument Number  Information      1  Azure Username    2  Azure Password    3  Azure Subscription ID    4  Azure Resource Group Name    5  Azure Location    6  Azure Public IP Identifier    7  Azure DNS Identifier    8  Azure Virtual Network Name    9  Azure Subnet Name    10  Azure NIC Name    11  Azure Virtual Machine Name    12  AWS CLI Key ID    13  AWS CLI Key    14  AWS Region    15  Batch Jobs Azure VM Can Handle Concurrently    16  Processors Needed Per Batch Job    17  Fraction Of Node Memory That One Task Will Utilize    18  S3 Location Where Output Should Be Uploaded To    19  Static IP Of Primary Node    20  For Compatibility With Scripts For AWS Always Make This Argument \"false\"    21  Type Of Azure VM To Spin Up", 
            "title": "Add A Microsoft Azure Machine To A Running Batch Processing Pipeline"
        }, 
        {
            "location": "/tutorials/#add_your_local_machine_to_a_running_batch_processing_pipeline", 
            "text": "Please note that it is a requirement that your hardware has a static IP address and is not being operated under any sort of scheduler.\nTo add your own hardware to a currently executing Aether pipeline, run bin/local.sh [args]  where  [args]  are\nthe following 9 arguments in sequential order:     Argument Number  Information      1  AWS CLI Key ID    2  AWS CLI Key    3  AWS Region    4  Batch Jobs Your Hardware Can Handle Concurrently    5  Processors Needed Per Batch Job    6  Fraction Of Node Memory That One Task Will Utilize    7  S3 Location Where Output Should Be Uploaded To    8  Static IP Of Primary Node    9  For Compatibility With Scripts For AWS Always Make This Argument \"false\"", 
            "title": "Add Your Local Machine To A Running Batch Processing Pipeline"
        }, 
        {
            "location": "/frequently_asked_questions/", 
            "text": "Frequently Asked Questions\n\n\n\n\nHow should I cite Aether or the associated website/paper?\n\n\nIf you use Aether or its associated resources, you should cite the paper:\n\n\nJacob M Luber, Braden T Tierney, Evan M Cofer, Chirag J Patel, Aleksandar D Kostic; Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics, Bioinformatics, , btx787, \nhttps://doi.org/10.1093/bioinformatics/btx787\n\n\n@article{doi:10.1093/bioinformatics/btx787,\nauthor = {Luber, Jacob M and Tierney, Braden T and Cofer, Evan M and Patel, Chirag J and Kostic, Aleksandar D},\ntitle = {Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics},\njournal = {Bioinformatics},\nvolume = {},\nnumber = {},\npages = {btx787},\nyear = {2017},\ndoi = {10.1093/bioinformatics/btx787},\nURL = { + http://dx.doi.org/10.1093/bioinformatics/btx787},\neprint = {/oup/backfile/content_public/journal/bioinformatics/pap/10.1093_bioinformatics_btx787/1/btx787.pdf}\n}\n\n\n\n\n\n\nWill you add support for cloud provider X?\n\n\nOur immediate goal is continue extending support for AWS and Azure - the most widely used cloud services - and improving Aether core features.\nHowever, we may eventually add support for other cloud platforms.\nFollow the project on \nGitHub\n to track updates.\nIf you have added support for a cloud provider that you use, consider filing an issue or submitting a pull request on \nGitHub\n.\n\n\n\n\nHow can I contribute to Aether?\n\n\nWe always welcome contributions.\nIf you would like to request a new feature, file a bug report, or have questions about documentation, you may want to file an issue on our \nGitHub issues page\n.\nIf you do file an issue, please read our \ncontribution guidelines\n, and use the issue template provided \nhere\n.\nFinally, if you have used Aether in your work, please let us know.\nWe are always excited to hear about how Aether is being applied!", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/frequently_asked_questions/#frequently_asked_questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/frequently_asked_questions/#how_should_i_cite_aether_or_the_associated_websitepaper", 
            "text": "If you use Aether or its associated resources, you should cite the paper:  Jacob M Luber, Braden T Tierney, Evan M Cofer, Chirag J Patel, Aleksandar D Kostic; Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics, Bioinformatics, , btx787,  https://doi.org/10.1093/bioinformatics/btx787  @article{doi:10.1093/bioinformatics/btx787,\nauthor = {Luber, Jacob M and Tierney, Braden T and Cofer, Evan M and Patel, Chirag J and Kostic, Aleksandar D},\ntitle = {Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics},\njournal = {Bioinformatics},\nvolume = {},\nnumber = {},\npages = {btx787},\nyear = {2017},\ndoi = {10.1093/bioinformatics/btx787},\nURL = { + http://dx.doi.org/10.1093/bioinformatics/btx787},\neprint = {/oup/backfile/content_public/journal/bioinformatics/pap/10.1093_bioinformatics_btx787/1/btx787.pdf}\n}", 
            "title": "How should I cite Aether or the associated website/paper?"
        }, 
        {
            "location": "/frequently_asked_questions/#will_you_add_support_for_cloud_provider_x", 
            "text": "Our immediate goal is continue extending support for AWS and Azure - the most widely used cloud services - and improving Aether core features.\nHowever, we may eventually add support for other cloud platforms.\nFollow the project on  GitHub  to track updates.\nIf you have added support for a cloud provider that you use, consider filing an issue or submitting a pull request on  GitHub .", 
            "title": "Will you add support for cloud provider X?"
        }, 
        {
            "location": "/frequently_asked_questions/#how_can_i_contribute_to_aether", 
            "text": "We always welcome contributions.\nIf you would like to request a new feature, file a bug report, or have questions about documentation, you may want to file an issue on our  GitHub issues page .\nIf you do file an issue, please read our  contribution guidelines , and use the issue template provided  here .\nFinally, if you have used Aether in your work, please let us know.\nWe are always excited to hear about how Aether is being applied!", 
            "title": "How can I contribute to Aether?"
        }, 
        {
            "location": "/contributions/", 
            "text": "Filing Bug Reports, Contributions, and Feature Requests\n\n\nWhether you need to report a bug, suggest new features, or give us feedback, make sure to follow the instructions on this page.\nThis will allow us to respond as quickly as possible.\n\n\n\n\nBug Reports\n\n\nIf you should encounter a bug, please follow the instructions below for reporting.\nThe more detailed and reproducible you example is, the more quickly it can be fixed.\n\n\n\n\nEnsure that your code is up-to-date by pulling the latest code from \nGitHub\n.\n\n\nCheck if anyone else has reported the same bug. If a similar bug has been reported, make sure to mention this in your report.\n\n\nInclude important information for reproducing the bug, like your operating system, your version of python, etc.\n\n\nAlthough it is not always possible, it is helpful if you can provide a script to exactly reproduce this bug.\n\n\nIf you fixed the bug, please consider submitting a pull request with the change.\n\n\nIf possible, use the \nbug\n label for the GitHub issue.\n\n\nFinally, use the template for issues and reports located \nhere\n.\n\n\n\n\n\n\nFeature Requests\n\n\nIf you think there is a feature that would greatly improve functionality or usability, consider filing an issue for it on \nGitHub\n.\nA good feature request will answer the following questions:\n\n\n\n\nWhat is this feature and how is it used?\n\n\nIs this feature useful for most users? Why?\n\n\nShould this be a feature (i.e., fully integrated into the main code base) or an add-on?\n\n\nIf possible, use the \nenhancement\n label for the GitHub issue.\n\n\nLastly, follow the template for issues found \nhere\n.\n\n\n\n\n\n\nDocumentation Questions\n\n\nIf documentation is unclear, please file an issue on \nGitHub\n, but follow these instructions:\n\n\n\n\nMake sure to follow the the proper protocol for filing issues as listed above, but, if possible, use the \nquestion\n label for the GitHub issue.\n\n\nAs with any issue, use the template for filing GitHub issues found \nhere\n.", 
            "title": "Contributions and Feedback"
        }, 
        {
            "location": "/contributions/#filing_bug_reports_contributions_and_feature_requests", 
            "text": "Whether you need to report a bug, suggest new features, or give us feedback, make sure to follow the instructions on this page.\nThis will allow us to respond as quickly as possible.", 
            "title": "Filing Bug Reports, Contributions, and Feature Requests"
        }, 
        {
            "location": "/contributions/#bug_reports", 
            "text": "If you should encounter a bug, please follow the instructions below for reporting.\nThe more detailed and reproducible you example is, the more quickly it can be fixed.   Ensure that your code is up-to-date by pulling the latest code from  GitHub .  Check if anyone else has reported the same bug. If a similar bug has been reported, make sure to mention this in your report.  Include important information for reproducing the bug, like your operating system, your version of python, etc.  Although it is not always possible, it is helpful if you can provide a script to exactly reproduce this bug.  If you fixed the bug, please consider submitting a pull request with the change.  If possible, use the  bug  label for the GitHub issue.  Finally, use the template for issues and reports located  here .", 
            "title": "Bug Reports"
        }, 
        {
            "location": "/contributions/#feature_requests", 
            "text": "If you think there is a feature that would greatly improve functionality or usability, consider filing an issue for it on  GitHub .\nA good feature request will answer the following questions:   What is this feature and how is it used?  Is this feature useful for most users? Why?  Should this be a feature (i.e., fully integrated into the main code base) or an add-on?  If possible, use the  enhancement  label for the GitHub issue.  Lastly, follow the template for issues found  here .", 
            "title": "Feature Requests"
        }, 
        {
            "location": "/contributions/#documentation_questions", 
            "text": "If documentation is unclear, please file an issue on  GitHub , but follow these instructions:   Make sure to follow the the proper protocol for filing issues as listed above, but, if possible, use the  question  label for the GitHub issue.  As with any issue, use the template for filing GitHub issues found  here .", 
            "title": "Documentation Questions"
        }
    ]
}